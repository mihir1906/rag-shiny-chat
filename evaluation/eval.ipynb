{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9aa358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd51c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc_id(metadata:dict,section:str)->str:\n",
    "    combined = f\"{section}-{metadata['text'][:20]}\"\n",
    "    hash_comb = hashlib.sha256(combined.encode())\n",
    "    hex_comb = hash_comb.hexdigest()[:10]\n",
    "    return hex_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e54302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e91135076b4382b2a9ba7de5486818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pth = Path(\"../weaviate_loader/documents\").resolve()\n",
    "documents = SimpleDirectoryReader(str(pth), recursive=True).load_data()\n",
    "\n",
    "splitter = MarkdownNodeParser()\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents, show_progress=True)\n",
    "\n",
    "texts = [node.get_content(metadata_mode=MetadataMode.NONE) for node in nodes]\n",
    "\n",
    "metadata_lst: list[dict[str, Any]] = []\n",
    "word = \"documents\"\n",
    "url = \"https://docs.pola.rs/user-guide\"\n",
    "\n",
    "for node in nodes:\n",
    "    node.get_content()\n",
    "    meta = node.metadata | {\n",
    "        \"text\": node.get_content(metadata_mode=MetadataMode.NONE)\n",
    "    }\n",
    "    file_path = node.metadata[\"file_path\"]\n",
    "    start_idx = file_path.find(word) + len(word)\n",
    "    link = url + file_path[start_idx:-8]\n",
    "    meta = meta | {\"link\": link} \n",
    "    doc_id = generate_doc_id(meta,file_path[start_idx:-8])\n",
    "    meta = meta | {\"id\":doc_id}\n",
    "    metadata_lst.append(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99623208",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a programmer familiar with the Polars user guide.\n",
    "Formulate 3 questions that you might ask based on this section of documentation.\n",
    "The section should contain the answer to the question and the questions should be complete and not too short.\n",
    "If possible, use as few words as possible from the section.\n",
    "\n",
    "The section:\n",
    "text {text}\n",
    "\n",
    "Provide the answer in parsable json without using code blocks\n",
    "\n",
    "[\"question1\",\"question2\",\"question3\"]\n",
    "\"\"\".strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01f32935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Extra data: line 2 column 521 (char 521)\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "client = ollama.AsyncClient(host=\"http://localhost:11434\")\n",
    "output_lst = []\n",
    "\n",
    "for i in range(0,40):\n",
    "    print(i)\n",
    "    meta = metadata_lst[i]\n",
    "    prompt = prompt_template.format(text=meta[\"text\"])\n",
    "    resp = await client.generate(model=\"gemma2:2b\",prompt=prompt)\n",
    "    ans = resp[\"response\"].replace(\"```json\",\"\").replace(\"```\",\"\")\n",
    "    try:\n",
    "        questions = json.loads(f\"{ans}\")\n",
    "        output_lst.append(meta | {\"question\":questions})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "953c494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\",\"w\") as f:\n",
    "    json.dump(output_lst,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091a44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store in vector db\n",
    "\n",
    "#retreive the question\n",
    "\n",
    "#compute relavance\n",
    "\n",
    "#compute metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0504dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from pydantic import BaseModel\n",
    "import httpx\n",
    "\n",
    "\n",
    "def batched(iterable, n):\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be at least one\")\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(islice(iterator, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "class TextEmbeddingsInference(BaseModel, Embeddings):\n",
    "    url: str\n",
    "    \"\"\"Url of text embeddings inference server\"\"\"\n",
    "    normalize: bool = True\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a Text Embeddings Inference server.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for batch in batched(texts, 8):\n",
    "            payload = {\n",
    "                \"inputs\": list(batch),\n",
    "                \"normalize\": self.normalize,\n",
    "                \"truncate\": True,\n",
    "            }\n",
    "            response = httpx.post(f\"{self.url}/embed\", json=payload).json()\n",
    "            embeddings.extend(response)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Compute query embeddings using a Text Embeddings Inference server.\n",
    "\n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "\n",
    "        Returns:\n",
    "            Embeddings for the text.\n",
    "        \"\"\"\n",
    "        return self.embed_documents([text])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "658cdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "WEAVIATE_HOST=\"localhost\"\n",
    "WEAVIATE_PORT = 8090\n",
    "TEI_HOST=\"localhost\"\n",
    "TEI_PORT=\"8080\"\n",
    "client = weaviate.connect_to_local(host=WEAVIATE_HOST, port=(WEAVIATE_PORT))\n",
    "tei_url = f\"http://{TEI_HOST}:{TEI_PORT}\"\n",
    "embeddings = TextEmbeddingsInference(url=tei_url, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fecc6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.get(\"document_collection\")\n",
    "relevance_lst = []\n",
    "for i in range(20):\n",
    "    for j in range(3):\n",
    "        query = output_lst[i][\"question\"][j]\n",
    "        vec = embeddings.embed_query(query)\n",
    "        result = collection.query.hybrid(\n",
    "                    query,\n",
    "                    alpha=0.5,\n",
    "                    vector=vec,\n",
    "                    limit=3,\n",
    "                    \n",
    "                )\n",
    "        props = [x.properties for x in result.objects]\n",
    "        props_copy = []\n",
    "        for p in props:\n",
    "            file_path = p[\"file_path\"]\n",
    "            word = \"documents\"\n",
    "            start_idx = file_path.find(word) + len(word)\n",
    "            new_p = p | {\"id\":generate_doc_id(p,file_path[start_idx:-8])}\n",
    "            props_copy.append(new_p)\n",
    "\n",
    "        relevance = [y[\"id\"]==output_lst[i][\"id\"] for y in props_copy]\n",
    "        relevance_lst.append(relevance)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a7b6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance):\n",
    "    cnt = 0\n",
    "    for i,v in enumerate(relevance):\n",
    "        if True in v:\n",
    "            cnt+=1\n",
    "    return cnt/len(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25d366fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(relevance):\n",
    "    ranks_lst = []\n",
    "    for v in relevance:\n",
    "        idx = v.index(True)+1 if True in v else 0\n",
    "        if idx==0:\n",
    "            rank = 0\n",
    "        else:\n",
    "            rank = 1/idx\n",
    "        ranks_lst.append(rank)\n",
    "\n",
    "    return sum(ranks_lst)/len(relevance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "641f6947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miky/.pyenv/versions/3.12.7/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=66 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "collection = client.collections.get(\"document_collection\")\n",
    "\n",
    "hit_rate_lst = []\n",
    "for alpha in [0.5,0.7,0.8,0.9,1]:\n",
    "    relevance_lst = []\n",
    "    for i in range(20):\n",
    "        for j in range(3):\n",
    "            query = output_lst[i][\"question\"][j]\n",
    "            vec = embeddings.embed_query(query)\n",
    "            result = collection.query.hybrid(\n",
    "                        query,\n",
    "                        alpha=alpha,\n",
    "                        vector=vec,\n",
    "                        limit=3,\n",
    "                        \n",
    "                    )\n",
    "            props = [x.properties for x in result.objects]\n",
    "            props_copy = []\n",
    "            for p in props:\n",
    "                file_path = p[\"file_path\"]\n",
    "                word = \"documents\"\n",
    "                start_idx = file_path.find(word) + len(word)\n",
    "                new_p = p | {\"id\":generate_doc_id(p,file_path[start_idx:-8])}\n",
    "                props_copy.append(new_p)\n",
    "\n",
    "            relevance = [y[\"id\"]==output_lst[i][\"id\"] for y in props_copy]\n",
    "            relevance_lst.append(relevance)\n",
    "    hit_rate_lst.append((alpha,hit_rate(relevance_lst),mrr(relevance_lst)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0888d3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5, 0.9, 0.7972222222222223),\n",
       " (0.7, 0.9166666666666666, 0.8222222222222223),\n",
       " (0.8, 0.9, 0.7972222222222223),\n",
       " (0.9, 0.9, 0.7583333333333333),\n",
       " (1, 0.8666666666666667, 0.7472222222222222)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16948218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [False, False, True],\n",
       " [False, False, False],\n",
       " [False, False, True],\n",
       " [False, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [False, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [False, False, False],\n",
       " [False, True, False],\n",
       " [False, False, False],\n",
       " [False, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [False, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [False, False, True],\n",
       " [False, False, False],\n",
       " [False, True, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [False, False, True],\n",
       " [False, False, True],\n",
       " [False, True, False],\n",
       " [False, False, True],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [False, True, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [True, False, False],\n",
       " [False, True, False],\n",
       " [True, False, False],\n",
       " [False, False, True],\n",
       " [True, False, False]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae706e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2 (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
