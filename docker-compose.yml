services:
  ollama:
    image: ollama/ollama
    volumes:
      - ./ollama/ollama:/root/.ollama
      - ./ollama_entrypoint.sh:/entrypoint.sh
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]
    env_file: ".env"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  server:
    build:
      context: backend
    ports:
      - "8000:8000"
    env_file: ".env"
    depends_on:
        - ollama
        - weaviate
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  ui:
    build:
      context: frontend
    ports:
      - "8070:8070"
    env_file: ".env"
    depends_on:
        - server

  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8090'
    - --scheme
    - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.25.4
    ports:
    - "8090:8090"
    - "50051:50051"
    volumes:
    - weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'

  text-embeddings-inference:
    ports:
      - 8080:8080
    volumes:
        - ./data:/data
    image: ghcr.io/huggingface/text-embeddings-inference:turing-1.5
    env_file:
      - .env
    command: --model-id $MODEL --pooling mean --port 8080
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                  - gpu
volumes:
  weaviate_data:








